\chapter{Detalles de implementación y experimentos}\label{chapter:implementation}

Este capítulo presenta los métodos, técnicas, detalles e hiperparámetros utilizados en la implementación general del modelo y específica de cada experimento.

\section*{Configuraciones generales}

El código desarrollado en la implementación de este modelo en púbico y se puede descargar en github en el repositorio \href{https://github.com/deborahfam/Thesis}{github.com/deborahfam/Thesis} en la carpeta llamada \textit{pipelines}. Cada pipeline es un archivo \textit{.ipynb}. Adelante se describen herramientas y tecnologías que fueron utilizadas en el proyecto:

\section*{Herramientas y Tecnologías}

\begin{itemize}
   \item \textbf{Frameworks de Desarrollo:}
   \begin{itemize}
       \item TensorFlow: Framework principal para la construcción y entrenamiento de modelos de machine learning y redes neuronales profundas.
   \end{itemize}

   \item \textbf{APIs y Librerías de TensorFlow:}
   \begin{itemize}
       \item Keras: API de alto nivel para la construcción y entrenamiento de modelos en TensorFlow.
       \item tf.keras.layers: Conjunto de capas predefinidas para la construcción de modelos.
       \item tf.keras.optimizers: Algoritmos de optimización como Adam y Adamax.
       \item tf.keras.metrics: Métricas de evaluación como categorical\_crossentropy.
       \item tf.keras.regularizers: Técnicas de regularización como L1 y L2.
       \item tf.keras.preprocessing.image: Herramientas de preprocesamiento y aumento de imágenes.
       \item tf.keras.models: Herramientas para la creación y carga de modelos.
   \end{itemize}

   \item \textbf{Librerías de Manipulación de Datos:}
   \begin{itemize}
       \item NumPy: Manejo de arrays y matrices de números n-dimensionales.
       \item Pandas: Análisis y manipulación de datos estructurados.
   \end{itemize}

   \item \textbf{Utilidades de Sistema y Archivos:}
   \begin{itemize}
       \item Shutil: Operaciones de manejo de archivos.
       \item OS: Interacción con el sistema operativo.
   \end{itemize}

   \item \textbf{Librerías de Visión por Computadora:}
   \begin{itemize}
       \item OpenCV (cv2): Procesamiento de imágenes y visión por computadora.
   \end{itemize}

   \item \textbf{Herramientas de Visualización de Datos:}
   \begin{itemize}
       \item Matplotlib: Creación de gráficos y visualizaciones.
       \item Seaborn: Visualización de datos estadísticos.
   \end{itemize}

   \item \textbf{Librerías de Evaluación de Modelos:}
   \begin{itemize}
       \item Scikit-learn (sklearn): Herramientas para selección de modelos y evaluación.
   \end{itemize}

   \item \textbf{Librerías de Procesamiento de Imágenes:}
   \begin{itemize}
       \item Pillow (PIL): Procesamiento de imágenes para aplicaciones Python.
   \end{itemize}

   \item \textbf{Herramientas de Progreso y Medición de Tiempo:}
   \begin{itemize}
       \item TQDM: Barras de progreso para loops.
       \item Time: Medición del tiempo de ejecución de código.
   \end{itemize}

   \item \textbf{Mejora de Interfaz y Presentación:}
   \begin{itemize}
       \item IPython: Mejoras interactivas y de visualización para Python, especialmente en cuadernos Jupyter.
   \end{itemize}
\end{itemize}


\subsection*{Carga, visualización y transformación de datos}

Lo primero realizado fue la importación y transformación los datos para el modelo. En el proceso de importación, para la carga de imágenes utilizamos \texttt{plt.imread} de la librería Matplotlib y \texttt{pd.read\_csv} de la librería Pandas para cargar los metadatos. Para la división del conjunto de datos en entrenamiento, validación y prueba se utilizó la función \texttt{train\_test\_split} de la librería Scikit-Learn con las variables \texttt{random\_state=123} y \texttt{shuffle=True}. Estas variables aseguran aleatoriedad en la división mezclando los datos antes de dividirlos y que la división sea reproducible. La división, como se expresó en capítulos anteriores es distinta en cada experimento y se especifican los parámetros utilizados luego en el capítulo.

Para la carga y procesamiento de imágenes se utilizó la clase \texttt{ImageDataGenerator} de Keras \brackcite{img_gen}. Esta clase facilita la creación de lotes de imágenes que se utilizan durante el entrenamiento y la evaluación de modelos de Machine Learning, especialmente en el contexto de redes neuronales. 

Las transformaciones aplicadas fueron las siguientes

\begin{table}[ht]
   \centering
   \begin{tabular}{lccc}
   \hline
   Parámetro & Descripción  & Valor \\ \hline
   rotation range & 	Rango de rotación & 20 grados \\
   width shift range & Rango de desplazamiento horizontal & 20\% \\
   height shift range & Rango de desplazamiento vertical & 20\% \\
   shear range & 	Rango de corte & 20\% \\
   zoom range & Rango de zoom & 20\% \\
   horizontal flip & Activación de volteo horizontal & verdadero \\
   fill mode & Modo de relleno para manejar los píxeles faltantes & nearest \\ \hline
   \end{tabular}
   \caption{Parámetros de aumento de datos.}
   \label{tab:data_augmentation_params}
   \end{table}
     

\section*{Preparación del modelo}

El siguiente fragmento de código siguiente detalla la creación y compilación de un modelo de aprendizaje profundo utilizando Keras, una API de alto nivel para construir y entrenar modelos en TensorFlow. El modelo se basa en EfficientNetB1. Para la compilación del mismo se hace uso de \texttt{model.compile}, con un callback personalizado para ajustar la tasa de aprendizaje y para entrenar utilizamos el \texttt{model.fit}.

\begin{lstlisting}[language=Python]
   model_name='EfficientNetB1'
   base_model=tf.keras.applications.EfficientNetB1(include_top=False, weights="imagenet",input_shape=img_shape, pooling='max')

   x=base_model.output
   x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)
   x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),
                  bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)
   x=Dropout(rate=.45, seed=123)(x)

   output=Dense(class_count, activation='softmax')(x)
   model=Model(inputs=base_model.input, outputs=output)
   model.compile(Adamax(learning_rate=.001), loss='categorical_crossentropy', metrics=['accuracy'])
\end{lstlisting}

\subsection*{Detalles de Hiperparámetros y Configuración}

\begin{itemize}
   \item \textbf{Arquitectura del Modelo:} Utilizamos EfficientNetB1 preentrenado con pesos de ImageNet como base.
   \item \textbf{Tasa de Aprendizaje Inicial:} Establecida en $0.001$.
   \item \textbf{Regularización:} Aplicación de regularización L2 con $\lambda = 0.016$ y regularización L1 con $\lambda = 0.006$ para los pesos y sesgos.
   \item \textbf{Dropout:} Se implementa una tasa de dropout del 45\% para mitigar el sobre-ajuste.
   \item \textbf{Dimensiones de la Imagen:} $224 \times 224 \times 3$.
   \item \textbf{Optimizador:} Adamax.
   \item \textbf{Métrica de Evaluación:} La precisión (\textit{accuracy}) se utiliza como la métrica de evaluación principal.
   \item \textbf{Callbacks:} Se incorpora un ajuste de tasa de aprendizaje (LRA) con los siguientes parámetros
   \item \begin{itemize}
      \item \textbf{Epochs:} $40$.
      \item \textbf{Patience:} 1.      
      \item \textbf{Stop Patience:} 3.
      \item \textbf{Threshold:}  0.9.
      \item \textbf{Factor:} 0.5.      
      \item \textbf{Dwell:} True
      \item \textbf{Freeze:} False  
      \item \textbf{Batches:} $train\_steps = (len(train_gen.labels)/batch_size)$
   \end{itemize}
\end{itemize}

\section{Capas adicionales y regularización}
   
   Para ajustar el modelo de EfficientNetB1 a nuestras necesidades, se añaden capas adicionales:
   
   \subsection{Normalización por lotes}
   
   Esta capa se define con un \textit{eje de normalización} establecido en $-1$, lo que indica que la normalización se aplica a lo largo del último eje en el tensor de entrada. Además, se configura un \textit{momentum} de $0.99$ y un valor de \textit{epsilon} de $0.001$. El alto valor de momentum ayuda a mantener la estabilidad de las medias y varianzas móviles a lo largo del entrenamiento, mientras que el pequeño valor de epsilon evita divisiones por cero, asegurando así cálculos numéricos estables \brackcite{regularization}.
   
   \subsection{Capa densa}
   
   Se integra una capa densa con $256$ neuronas, que juega un papel clave en la síntesis de las características aprendidas por el modelo. Se utiliza un regularizador L2 con un \textit{lambda} de $0.016$ para los pesos de la capa, lo que ayuda a penalizar y controlar el tamaño de los pesos, reduciendo así el riesgo de sobre-ajuste. Además, tanto el regularizador de actividad como el regularizador de bias se configuran con un regularizador L1 con un \textit{lambda} de $0.006$. Este enfoque impone una penalización en los pesos y los sesgos, promoviendo un modelo más simple y disperso. La función de activación utilizada es \textit{ReLU} , conocida por su eficacia en la introducción de no linealidad en el modelo, lo que permite aprender relaciones complejas entre las características \brackcite{dense}.
   
   \subsection{Dropout}
   
   En la arquitectura del modelo, se integra una capa de dropout para aumentar la robustez y prevenir el sobre-ajuste. Esta capa se configura con una tasa de desactivación del 45\% , lo que significa que, durante el entrenamiento, el 45\% de las neuronas se desactivarán aleatoriamente en cada paso. 
   
   En una primera iteración del algoritmo tuvo una tasa de desactivación más baja. Luego de varias iteraciones se concluyó que se necesitaba un algoritmo de clasificación que estudiara más a detalle la data fomentando así una mejor generalización y reduciendo el riesgo de sobre-ajuste en el proceso de aprendizaje. Basándonos en esto aumentamos la tasa y obtuvimos mejores resultados.
   
   Para asegurar la reproducibilidad, se establece una semilla(seed) $123$. Esta introducción de aleatoriedad ayuda a que el modelo no dependa excesivamente de ninguna característica o neurona específica \brackcite{dropout}.
   
   \subsection{Capa de salida}
   
   La capa de salida utiliza una activación \textit{softmax} para transformar las salidas del modelo en probabilidades de pertenencia a cada clase. Esto, clasifica las entradas en categorías distintas, proporcionando probabilidades para cada clase, lo cual es esencial en la clasificación multi-clase.
   
   \subsection{Optimización}
   
   Para el proceso de entrenamiento, se utiliza el optimizador \textit{Adamax} \brackcite{adamax}. Este es una variante del conocido optimizador Adam, que combina las ventajas de los métodos adaptativos de tasa de aprendizaje con una implementación más robusta en entornos con gradientes dispersos, lo cual es común en imágenes médicas 
   
   La función de pérdida elegida es la \textit{categorical crossentropy} \brackcite{vitalflux_categorical_crossentropy}, idónea para problemas de clasificación multi-clase. Se configura el modelo para minimizarla y se rastrea la precisión como métrica principal.

\section{Experimentos}

Como se había expresado al inicio del capítulo se llevaron a cabo una serie de experimentos de los cuales analizaremos los siguientes para la distribución y normalización de datos. La metodología general para ambos experimentos coincide, en ambos se utiliza una red convolucional, EfficientNetB1 y capas adicionales.

\subsection{Experimento 1: Evaluación de la eficiencia de la división asimétrica de datos en la clasificación de imágenes de cáncer de piel}

Este experimento se centra en una división de datos altamente asimétrica, con un enfoque predominante en el conjunto de entrenamiento. La técnica de \textit{dummy split} se emplea para mantener proporciones consistentes entre los conjuntos de validación y prueba. A esta división se le aplica luego un determinado peso para que el algoritmo preste especial atención a las clases menos representadas.

\subsection{Experimento 2: Análisis de la estratificación de datos en la clasificación de imágenes de cáncer de piel}

Este experimento explora la estratificación de datos para mantener una distribución uniforme de etiquetas en cada conjunto de datos. La proporción de los conjuntos de datos es más equilibrada en comparación con el Experimento 1, lo que ofrece \textit{insights} sobre la importancia de la distribución equitativa de datos en el entrenamiento y evaluación de modelos.
\section{Distribución de datos en los experimentos}

\subsection{Experimento 1: Evaluación de la división asimétrica de datos}

En este experimento de las herramientas que se utiliza para hacer la división se hace uso del \textit{dummy split}  para mantener la proporción deseada entre validación y prueba. La distribución de datos fue la siguiente:

\begin{table}[ht]
   \centering
   \begin{tabular}{lcc}
   \hline
   \textbf{Diagnostic category} & \textbf{Porcentaje} & \textbf{Cantidad de datos} \\
   \hline
   Entrenamiento       & 95\% & $9514$ \\
   Validación      & 2.5\% & $251$  \\
   Prueba      & 2.5\% & $251$  \\ \hline
   \end{tabular}
   \caption{Distribución de los conjuntos de entrenamiento, validación y prueba.}
   \label{table:data_distribution_e1}
   \end{table}

   Por lo que la division de datos por clase quedaría de la siguiente forma:

   \begin{table}[ht]
      \centering
      \begin{tabular}{lccc}
      \hline
      \textbf{Diagnostic category} & \textbf{Entrenamiento} & \textbf{Validación} & \textbf{Prueba} \\
      \hline
      NV       & 300 & 158 & 163 \\
      MEL      & 300 & 25  & 35  \\
      BKL      & 300 & 34  & 30  \\
      DF       & 115 & 5   & 3   \\
      AKIEC    & 300 & 11  & 7   \\
      BCC      & 300 & 16  & 10  \\
      VASC     & 142 & 1   & 3   \\ \hline
      \end{tabular}
      \caption{Experimento 1: Distribución de imágenes de cáncer de piel en los conjuntos de entrenamiento, test y validación.}
      \label{table:train_test_validate_e1}
      \end{table}
   
\subsubsection*{Generadores de datos y preprocesamiento}

Se establece para este un tamaño objetivo de muestras por clase ($300$ en este caso), y se utiliza un bucle para iterar a través de cada clase única.

\begin{table}[ht]
   \centering
   \begin{tabular}{lccc}
   \hline
   Diagnostic category & Sampling  \\ \hline
   AKIEC & 300 \\
   BCC & 300 \\
   BKL & 300 \\
   DF & 115 \\
   MEL & 300 \\
   NV & 300 \\
   VASC & 142 \\ \hline
   \end{tabular}
   \caption{Distribución de muestras por categoría después del sobre-muestreo.}
   \label{tab:sampling_distribution_1}
   \end{table}


Como se evidencia anteriormente, al separar la data en clases el conjunto se mantiene desbalanceado. Se hace necesario aplicar un método llamado \textit{Class Weighting} para compensar este desbalanceo. Este método consiste en asignar un peso a cada clase inversamente proporcional a su frecuencia. De esta manera, las clases con menor representación tendrán un mayor peso y las clases con mayor representación tendrán un menor peso. Esto permite que el modelo se entrene de manera más equilibrada y que no se sesgue hacia las clases con mayor representación.

\begin{table}[ht]
   \centering
   \begin{tabular}{lccc}
   \hline
   Diagnostic category & Sampling  & Weighting\\ \hline
   AKIEC & 300 & 1.00\\
   BCC & 300 & 1.00\\
   BKL & 300 & 1.00\\
   DF & 115 & 2.60\\
   MEL & 300 & 1.00\\
   NV & 300 & 1.00\\
   VASC & 142 & 2.11\\ \hline
   \end{tabular}
   \caption{Distribución de muestras con peso asignado.}
   \label{tab:weighting_distribution}
   \end{table}


\subsection{Experimento 2: Análisis de la estratificación de datos}

Se utiliza \textit{stratify} en ambas divisiones para mantener la distribución de etiquetas \textit{label} en cada conjunto. Se calcula la proporción del conjunto de prueba sobre la suma del conjunto de test y el de validación. La distribución de datos fue la siguiente:

   \begin{table}[ht]
      \centering
      \begin{tabular}{lcc}
      \hline
      \textbf{Diagnostic category} & \textbf{Porcentaje} & \textbf{Cantidad de datos} \\
      \hline
      Entrenamiento       & 70\% &  $7010$ \\
      Validación      & 15\% & $1502$  \\
      Prueba      & 15\% & $1503$  \\ \hline
      \end{tabular}
      \caption{Distribución de los conjuntos de entrenamiento, validación y prueba.}
      \label{table:data_distribution_e2}
      \end{table}

Luego este quedaría distribuido en datos de entrenamiento, test y  validación por clase de la siguiente forma:
   \begin{table}[ht]
      \centering
      \begin{tabular}{lccc}
      \hline
      \textbf{Diagnostic category} & \textbf{Training} & \textbf{Validation} & \textbf{Testing} \\
      \hline
      NV    & 500 & 1006 & 1006 \\
      MEL   & 500 & 167  & 167  \\
      BKL   & 500 & 165  & 165  \\
      DF    & 500 & 17   & 17   \\
      AKIEC & 500 & 49   & 49   \\
      BCC   & 500 & 77   & 77   \\
      VASC  & 500 & 21   & 22   \\
      \hline
      \end{tabular}
      \caption{Experimento 2: Distribución de imágenes de cáncer de piel en los conjuntos de entrenamiento, test y validación.}
      \label{tab:train_test_validate_e2}
      \end{table}
 
\subsubsection*{Generadores de datos y preprocesamiento}

Se establece también un tamaño objetivo de muestras por clase ($500$). Se itera sobre cada clase, y se realiza un re-muestreo con reemplazo para grupos menores al tamaño deseado y sin reemplazo para los grupos que ya alcanzan o superan el tamaño deseado. 
En este experimento, a diferencia del anterior, en cada clase se alcanza la misma cantidad de muestras, por lo que no es necesario aplicar \textit{Class Weighting}.